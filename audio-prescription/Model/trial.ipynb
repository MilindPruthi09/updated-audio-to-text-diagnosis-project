{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x219477755a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      ",\n",
      "world\n",
      "!\n",
      "How\n",
      "are\n",
      "you\n",
      "today\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Hello, world! How are you today?\")\n",
    "\n",
    "# Iterate over tokens in the processed text\n",
    "for token in doc:\n",
    "    print(token.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "India GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process a text\n",
    "# doc = nlp(\"Apple is a tech company based in Cupertino, California.\")\n",
    "doc = nlp(\"Apple is having headache in India.\")\n",
    "\n",
    "# Iterate over entities in the processed text\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Symptoms:  ['name', 'headache', 'body', 'pain']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# The text to be processed\n",
    "# sentence = \"I have a headache and a runny nose.\"\n",
    "\n",
    "sentence=\"Hi my name is Milind and I am facing headache and body pain\"\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(sentence)\n",
    "\n",
    "symptoms = []\n",
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        symptoms.append(token.text)\n",
    "\n",
    "print(\"Extracted Symptoms: \", symptoms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"en_ewt\" for language \"en\".\n",
      "Would you like to download the models for: en_ewt now? (Y/n)\n",
      "\n",
      "Default download directory: C:\\Users\\Rajeev Kumar\\stanfordnlp_resources\n",
      "Hit enter to continue or type an alternate directory.\n",
      "\n",
      "Downloading models for: en_ewt\n",
      "Download location: C:\\Users\\Rajeev Kumar\\stanfordnlp_resources\\en_ewt_models.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 67.1M/235M [00:21<00:54, 3.09MB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstanfordnlp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Load the small English model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m stanfordnlp\u001b[39m.\u001b[39;49mdownload(\u001b[39m'\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m nlp \u001b[39m=\u001b[39m stanfordnlp\u001b[39m.\u001b[39mPipeline(processors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtokenize,mwt,pos\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[39m# The text to be processed\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanfordnlp\\utils\\resources.py:136\u001b[0m, in \u001b[0;36mdownload\u001b[1;34m(download_label, resource_dir, confirm_if_exists, force, version)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39melif\u001b[39;00m download_label \u001b[39min\u001b[39;00m default_treebanks:\n\u001b[0;32m    135\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUsing the default treebank \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdefault_treebanks[download_label]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m for language \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdownload_label\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 136\u001b[0m     download_ud_model(default_treebanks[download_label], resource_dir\u001b[39m=\u001b[39;49mresource_dir,\n\u001b[0;32m    137\u001b[0m                       confirm_if_exists\u001b[39m=\u001b[39;49mconfirm_if_exists, force\u001b[39m=\u001b[39;49mforce, version\u001b[39m=\u001b[39;49mversion)\n\u001b[0;32m    138\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThe language or treebank \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdownload_label\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m is not currently supported by this function. Please try again with other languages or treebanks.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanfordnlp\\utils\\resources.py:106\u001b[0m, in \u001b[0;36mdownload_ud_model\u001b[1;34m(lang_name, resource_dir, should_unzip, confirm_if_exists, force, version)\u001b[0m\n\u001b[0;32m    104\u001b[0m default_chunk_size \u001b[39m=\u001b[39m \u001b[39m67108864\u001b[39m\n\u001b[0;32m    105\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39mfile_size, unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m, unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m--> 106\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39mdefault_chunk_size):\n\u001b[0;32m    107\u001b[0m         \u001b[39mif\u001b[39;00m chunk:\n\u001b[0;32m    108\u001b[0m             f\u001b[39m.\u001b[39mwrite(chunk)\n",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    463\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 465\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    467\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "# Load the small English model\n",
    "stanfordnlp.download('en')\n",
    "nlp = stanfordnlp.Pipeline(processors=\"tokenize,mwt,pos\")\n",
    "\n",
    "# The text to be processed\n",
    "sentence = \"I have a headache and a runny nose.\"\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(sentence)\n",
    "\n",
    "symptoms = []\n",
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        if word.upos == \"NOUN\":\n",
    "            symptoms.append(word.text)\n",
    "\n",
    "print(\"Extracted Symptoms: \", symptoms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pattern\\text\\__init__.py:609\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(path, encoding, comment)\u001b[0m\n\u001b[0;32m    608\u001b[0m         \u001b[39myield\u001b[39;00m line\n\u001b[1;32m--> 609\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m sentence \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mI have a headache and a runny nose.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[39m# Perform POS tagging\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m tagged_words \u001b[39m=\u001b[39m tag(sentence)\n\u001b[0;32m      9\u001b[0m symptoms \u001b[39m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m word, pos \u001b[39min\u001b[39;00m tagged_words:\n",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pattern\\text\\en\\__init__.py:188\u001b[0m, in \u001b[0;36mtag\u001b[1;34m(s, tokenize, encoding, **kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Returns a list of (token, tag)-tuples from the given string.\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    187\u001b[0m tags \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 188\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m parse(s, tokenize, \u001b[39mTrue\u001b[39;00m, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m, encoding, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\u001b[39m.\u001b[39msplit():\n\u001b[0;32m    189\u001b[0m     \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m sentence:\n\u001b[0;32m    190\u001b[0m         tags\u001b[39m.\u001b[39mappend((token[\u001b[39m0\u001b[39m], token[\u001b[39m1\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pattern\\text\\en\\__init__.py:169\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(s, *args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(s, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    167\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Returns a tagged Unicode string.\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39mparse(s, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pattern\\text\\__init__.py:1172\u001b[0m, in \u001b[0;36mParser.parse\u001b[1;34m(self, s, tokenize, tags, chunks, relations, lemmata, encoding, **kwargs)\u001b[0m\n\u001b[0;32m   1170\u001b[0m \u001b[39m# Tagger (required by chunker, labeler & lemmatizer).\u001b[39;00m\n\u001b[0;32m   1171\u001b[0m \u001b[39mif\u001b[39;00m tags \u001b[39mor\u001b[39;00m chunks \u001b[39mor\u001b[39;00m relations \u001b[39mor\u001b[39;00m lemmata:\n\u001b[1;32m-> 1172\u001b[0m     s[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfind_tags(s[i], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1173\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1174\u001b[0m     s[i] \u001b[39m=\u001b[39m [[w] \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m s[i]]\n",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pattern\\text\\en\\__init__.py:114\u001b[0m, in \u001b[0;36mParser.find_tags\u001b[1;34m(self, tokens, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtagset\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m UNIVERSAL:\n\u001b[0;32m    113\u001b[0m     kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mmap\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlambda\u001b[39;00m token, tag: penntreebank2universal(token, tag))\n\u001b[1;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m _Parser\u001b[39m.\u001b[39mfind_tags(\u001b[39mself\u001b[39m, tokens, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pattern\\text\\__init__.py:1112\u001b[0m, in \u001b[0;36mParser.find_tags\u001b[1;34m(self, tokens, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Annotates the given list of tokens with part-of-speech tags.\u001b[39;00m\n\u001b[0;32m   1109\u001b[0m \u001b[39m    Returns a list of tokens, where each token is now a [word, tag]-list.\u001b[39;00m\n\u001b[0;32m   1110\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1111\u001b[0m \u001b[39m# [\"The\", \"cat\", \"purs\"] => [[\"The\", \"DT\"], [\"cat\", \"NN\"], [\"purs\", \"VB\"]]\u001b[39;00m\n\u001b[1;32m-> 1112\u001b[0m \u001b[39mreturn\u001b[39;00m find_tags(tokens,\n\u001b[0;32m   1113\u001b[0m             lexicon \u001b[39m=\u001b[39;49m kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mlexicon\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlexicon \u001b[39mor\u001b[39;49;00m {}),\n\u001b[0;32m   1114\u001b[0m               model \u001b[39m=\u001b[39;49m kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel),\n\u001b[0;32m   1115\u001b[0m          morphology \u001b[39m=\u001b[39;49m kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmorphology\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmorphology),\n\u001b[0;32m   1116\u001b[0m             context \u001b[39m=\u001b[39;49m kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext),\n\u001b[0;32m   1117\u001b[0m            entities \u001b[39m=\u001b[39;49m kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mentities\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mentities),\n\u001b[0;32m   1118\u001b[0m            language \u001b[39m=\u001b[39;49m kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mlanguage\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlanguage),\n\u001b[0;32m   1119\u001b[0m             default \u001b[39m=\u001b[39;49m kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mdefault\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdefault),\n\u001b[0;32m   1120\u001b[0m                 \u001b[39mmap\u001b[39;49m \u001b[39m=\u001b[39;49m kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmap\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m))\n",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pattern\\text\\__init__.py:1540\u001b[0m, in \u001b[0;36mfind_tags\u001b[1;34m(tokens, lexicon, model, morphology, context, entities, default, language, map, **kwargs)\u001b[0m\n\u001b[0;32m   1538\u001b[0m \u001b[39m# Tag named entities.\u001b[39;00m\n\u001b[0;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m entities \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1540\u001b[0m     tagged \u001b[39m=\u001b[39m entities\u001b[39m.\u001b[39;49mapply(tagged)\n\u001b[0;32m   1541\u001b[0m \u001b[39m# Map tags with a custom function.\u001b[39;00m\n\u001b[0;32m   1542\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mmap\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pattern\\text\\__init__.py:976\u001b[0m, in \u001b[0;36mEntities.apply\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    972\u001b[0m \u001b[39mif\u001b[39;00m RE_ENTITY1\u001b[39m.\u001b[39mmatch(w) \\\n\u001b[0;32m    973\u001b[0m \u001b[39mor\u001b[39;00m RE_ENTITY2\u001b[39m.\u001b[39mmatch(w) \\\n\u001b[0;32m    974\u001b[0m \u001b[39mor\u001b[39;00m RE_ENTITY3\u001b[39m.\u001b[39mmatch(w):\n\u001b[0;32m    975\u001b[0m     tokens[i][\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtag\n\u001b[1;32m--> 976\u001b[0m \u001b[39mif\u001b[39;00m w \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m:\n\u001b[0;32m    977\u001b[0m     \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m[w]:\n\u001b[0;32m    978\u001b[0m         \u001b[39m# Look ahead to see if successive words match the named entity.\u001b[39;00m\n\u001b[0;32m    979\u001b[0m         e, tag \u001b[39m=\u001b[39m (e[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m e[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mupper()) \u001b[39mif\u001b[39;00m e[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cmd \u001b[39melse\u001b[39;00m (e, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pattern\\text\\__init__.py:382\u001b[0m, in \u001b[0;36mlazydict.__contains__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__contains__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m--> 382\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lazy(\u001b[39m\"\u001b[39;49m\u001b[39m__contains__\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pattern\\text\\__init__.py:368\u001b[0m, in \u001b[0;36mlazydict._lazy\u001b[1;34m(self, method, *args)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" If the dictionary is empty, calls lazydict.load().\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[39m    Replaces lazydict.method() with dict.method() and calls it.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mdict\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 368\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    369\u001b[0m     \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, method, types\u001b[39m.\u001b[39mMethodType(\u001b[39mgetattr\u001b[39m(\u001b[39mdict\u001b[39m, method), \u001b[39mself\u001b[39m))\n\u001b[0;32m    370\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mdict\u001b[39m, method)(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\Rajeev Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pattern\\text\\__init__.py:959\u001b[0m, in \u001b[0;36mEntities.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    957\u001b[0m     \u001b[39m# [\"Alexander\", \"the\", \"Great\", \"PERS\"]\u001b[39;00m\n\u001b[0;32m    958\u001b[0m     \u001b[39m# {\"alexander\": [[\"alexander\", \"the\", \"great\", \"pers\"], ...]}\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m _read(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath):\n\u001b[0;32m    960\u001b[0m         x \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39msplit()]\n\u001b[0;32m    961\u001b[0m         \u001b[39mdict\u001b[39m\u001b[39m.\u001b[39msetdefault(\u001b[39mself\u001b[39m, x[\u001b[39m0\u001b[39m], [])\u001b[39m.\u001b[39mappend(x)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "# from pattern.en import tag\n",
    "from pattern.text.en import tag\n",
    "# The text to be processed\n",
    "sentence = \"I have a headache and a runny nose.\"\n",
    "\n",
    "# Perform POS tagging\n",
    "tagged_words = tag(sentence)\n",
    "\n",
    "symptoms = []\n",
    "for word, pos in tagged_words:\n",
    "    if pos == \"NN\":\n",
    "        symptoms.append(word)\n",
    "\n",
    "print(\"Extracted Symptoms: \", symptoms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Symptoms:  ['sore', 'throat', 'swollen', 'glands']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# The text to be processed\n",
    "sentence = \"I have a sore throat and swollen glands.\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(sentence)\n",
    "\n",
    "symptoms = []\n",
    "for word, pos in blob.tags:\n",
    "    if pos in [\"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\", \"NN\", \"NNS\"]:\n",
    "        symptoms.append(word)\n",
    "\n",
    "print(\"Extracted Symptoms: \", symptoms)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Symptoms:  [['joints'], ['difficulty', 'walking']]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# The text to be processed\n",
    "sentence = \"I have pain in my joints and difficulty walking.\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(sentence)\n",
    "\n",
    "symptoms = []\n",
    "i = 0\n",
    "while i < len(blob.tags):\n",
    "    word, pos = blob.tags[i]\n",
    "    if pos in [\"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\"]:\n",
    "        j = i + 1\n",
    "        while j < len(blob.tags) and blob.tags[j][1] in [\"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\"]:\n",
    "            j += 1\n",
    "        symptoms.append([word for word, pos in blob.tags[i:j]])\n",
    "        i = j - 1\n",
    "    elif pos in [\"NN\",\"NNS\"]:\n",
    "        j = i + 1\n",
    "        while j < len(blob.tags) and blob.tags[j][1] in [\"NN\", \"NNS\"]:\n",
    "            j += 1\n",
    "        symptoms.append([word for word, pos in blob.tags[i:j]])\n",
    "        i = j - 1\n",
    "    i += 1\n",
    "\n",
    "print(\"Extracted Symptoms: \", symptoms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Symptoms:  [['pain'], ['joints'], ['difficulty'], ['walking']]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# The text to be processed\n",
    "sentence = \"I have pain in my joints and difficulty walking.\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(sentence)\n",
    "\n",
    "symptoms = []\n",
    "current_symptom = []\n",
    "for word, pos in blob.tags:\n",
    "    if word == \"pain\":\n",
    "        current_symptom.append(word)\n",
    "    elif pos in [\"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\"]:\n",
    "        current_symptom.append(word)\n",
    "    elif pos in [\"NN\", \"NNS\"]:\n",
    "        if current_symptom:\n",
    "            symptoms.append(current_symptom)\n",
    "            current_symptom = []\n",
    "        current_symptom.append(word)\n",
    "    elif pos == \"CC\":\n",
    "        symptoms.append(current_symptom)\n",
    "        current_symptom = []\n",
    "\n",
    "if current_symptom:\n",
    "    symptoms.append(current_symptom)\n",
    "\n",
    "print(\"Extracted Symptoms: \", symptoms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Symptoms:  [['have', 'pain', 'joints'], ['difficulty', 'walking']]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# The text to be processed\n",
    "sentence = \"I have pain in my joints and difficulty walking.\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(sentence)\n",
    "\n",
    "symptoms = []\n",
    "current_symptom = []\n",
    "for word, pos in blob.tags:\n",
    "    if pos in [\"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]:\n",
    "        current_symptom.append(word)\n",
    "    elif pos in [\"NN\", \"NNS\"]:\n",
    "        current_symptom.append(word)\n",
    "    elif pos == \"CC\":\n",
    "        if current_symptom:\n",
    "            symptoms.append(current_symptom)\n",
    "            current_symptom = []\n",
    "\n",
    "if current_symptom:\n",
    "    symptoms.append(current_symptom)\n",
    "\n",
    "print(\"Extracted Symptoms: \", symptoms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Symptoms:  [['developed', 'cough'], ['fever', 'returning', 'trip']]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# The text to be processed\n",
    "sentence = \"I have developed a cough and a fever after returning from my trip.\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(sentence)\n",
    "\n",
    "symptoms = []\n",
    "current_symptom = []\n",
    "for word, pos in blob.tags:\n",
    "    if pos in [\"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]:\n",
    "        if word == \"have\":\n",
    "            continue\n",
    "        current_symptom.append(word)\n",
    "    elif pos in [\"NN\", \"NNS\"]:\n",
    "        current_symptom.append(word)\n",
    "    elif pos == \"CC\":\n",
    "        if current_symptom:\n",
    "            symptoms.append(current_symptom)\n",
    "            current_symptom = []\n",
    "\n",
    "if current_symptom:\n",
    "    symptoms.append(current_symptom)\n",
    "\n",
    "print(\"Extracted Symptoms: \", symptoms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Rajeev\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Symptoms:  [['developed', 'cough', 'fever', 'returning', 'trip']]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# The text to be processed\n",
    "sentence = \"I have developed a cough and a fever after returning from my trip.\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(sentence)\n",
    "\n",
    "symptoms = []\n",
    "current_symptom = []\n",
    "for word, pos in blob.tags:\n",
    "    if word in stop_words:\n",
    "        continue\n",
    "    if pos in [\"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]:\n",
    "        current_symptom.append(word)\n",
    "    elif pos in [\"NN\", \"NNS\"]:\n",
    "        current_symptom.append(word)\n",
    "    elif pos == \"CC\":\n",
    "        if current_symptom:\n",
    "            symptoms.append(current_symptom)\n",
    "            current_symptom = []\n",
    "\n",
    "if current_symptom:\n",
    "    symptoms.append(current_symptom)\n",
    "\n",
    "print(\"Extracted Symptoms: \", symptoms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cough', 'fever']\n"
     ]
    }
   ],
   "source": [
    "def extract_symptoms(sentence):\n",
    "    # Create a TextBlob object\n",
    "    blob = TextBlob(sentence)\n",
    "    \n",
    "    # List of words associated with symptoms\n",
    "    symptom_words = ['pain', 'fever', 'cough', 'headache', 'fatigue', 'nausea', 'vomiting', 'diarrhea']\n",
    "    \n",
    "    # List of words associated with non-symptoms\n",
    "    non_symptom_words = ['I', 'have', 'been', 'with', 'for', 'since', 'yesterday', 'last', 'week']\n",
    "    \n",
    "    symptoms = []\n",
    "    current_symptom = []\n",
    "    for word in blob.words:\n",
    "        if word in non_symptom_words:\n",
    "            continue\n",
    "        if word in symptom_words:\n",
    "            symptoms.append(word)\n",
    "    \n",
    "    return symptoms\n",
    "\n",
    "sentence = \"I have developed a cough and a fever after returning from my trip.\"\n",
    "print(extract_symptoms(sentence))\n",
    "# Output: ['cough', 'fever']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Symptoms:  ['sore throat', 'throat', 'swollen']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentence = \"I have a sore throat and swollen glands.\"\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(sentence)\n",
    "\n",
    "symptoms = []\n",
    "for i, (word, pos) in enumerate(blob.tags):\n",
    "    if word in stop_words:\n",
    "        continue\n",
    "    if pos in [\"JJ\", \"IN\", \"NN\",\"NN\",\"VBN\"]:\n",
    "        if i+1 < len(blob.tags) and blob.tags[i+1][1] == \"NN\":\n",
    "            if word + \" \" + blob.tags[i+1][0] not in symptoms:\n",
    "                symptoms.append(word + \" \" + blob.tags[i+1][0])\n",
    "        else:\n",
    "            if word not in symptoms:\n",
    "                symptoms.append(word)\n",
    "\n",
    "print(\"Extracted Symptoms: \", symptoms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "486e0d5a79acdbfffd563ee7a67a93a5017bd2a4f66495483a69f0245c8a4a6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
